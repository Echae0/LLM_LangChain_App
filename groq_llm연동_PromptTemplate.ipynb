{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ead71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5f628d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a61086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe은 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b49884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001A6B04AEF00> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A6B04E6720> root_client=<openai.OpenAI object at 0x000001A6B04445F0> root_async_client=<openai.AsyncOpenAI object at 0x000001A6B04E6990> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53cd1e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 LLM을 애플리케이션에 통합하고, 모델의 성능을 모니터링하며, 필요에 따라 모델을 업데이트할 수 있습니다.\\n\\nLangServe의 주요 기능은 다음과 같습니다:\\n\\n1. **LLM 통합**: LangServe는 다양한 LLM을 지원하며, 개발자는 LangServe를 사용하여 LLM을 애플리케이션에 쉽게 통합할 수 있습니다. LangServe는 LLM의 입력과 출력을 표준화하여 개발자가 모델과 상호 작용하는 것을 더 쉽게 만들어줍니다.\\n\\n2. **모니터링 및 로그 분석**: LangServe는 LLM의 성능을 실시간으로 모니터링하고 로그를 분석할 수 있는 기능을 제공합니다. 이를 통해 개발자는 모델의 성능을 추적하고, 문제가 발생하면 빠르게 식별하고 해결할 수 있습니다.\\n\\n3. **모델 업데이트**: LangServe는 LLM을 쉽게 업데이트할 수 있는 기능을 제공합니다. 개발자는 새로운 모델 버전으로 업데이트하거나, 다른 모델로 교체할 수 있습니다.\\n\\n4. **접근 제어 및 인증**: LangServe는 접근 제어 및 인증 기능을 제공하여 모델에 대한 접근을 제한하고, 인증된 사용자만 모델을 사용할 수 있도록 합니다.\\n\\n5. **확장성**: LangServe는 수평 확장성을 지원하여 대규모 트래픽을 처리할 수 있습니다. 이를 통해 개발자는 대규모 사용자를 지원하면서도 모델의 성능을 유지할 수 있습니다.\\n\\n6. **다중 모델 지원**: LangServe는 다중 모델을 지원하여 여러 모델을 동시에 배포하고 관리할 수 있습니다. 이를 통해 개발자는 다양한 모델을 테스트하고 비교할 수 있습니다.\\n\\nLangServe를 사용하면 개발자는 LLM을 더 쉽게 배포하고 관리할 수 있으며, 모델의 성능을 모니터링하고 개선할 수 있습니다. LangServe는 오픈 소스 라이브러리이므로 무료로 사용할 수 있으며, 개발자는 LangServe를 자신의 프로젝트에 쉽게 통합할 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 398, 'prompt_tokens': 30, 'total_tokens': 428, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.333295406, 'prompt_time': 0.003360494, 'completion_time': 0.989872208, 'total_time': 0.993232702}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-dfd7445e-116f-4040-ace1-dd899dfb564d', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--9dd5271f-0fec-41aa-9c77-e5bed9197595-0' usage_metadata={'input_tokens': 30, 'output_tokens': 398, 'total_tokens': 428, 'input_token_details': {}, 'output_token_details': {}}\n",
      "응답: LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 LLM을 애플리케이션에 통합하고, 모델의 성능을 모니터링하며, 필요에 따라 모델을 업데이트할 수 있습니다.\n",
      "\n",
      "LangServe의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **LLM 통합**: LangServe는 다양한 LLM을 지원하며, 개발자는 LangServe를 사용하여 LLM을 애플리케이션에 쉽게 통합할 수 있습니다. LangServe는 LLM의 입력과 출력을 표준화하여 개발자가 모델과 상호 작용하는 것을 더 쉽게 만들어줍니다.\n",
      "\n",
      "2. **모니터링 및 로그 분석**: LangServe는 LLM의 성능을 실시간으로 모니터링하고 로그를 분석할 수 있는 기능을 제공합니다. 이를 통해 개발자는 모델의 성능을 추적하고, 문제가 발생하면 빠르게 식별하고 해결할 수 있습니다.\n",
      "\n",
      "3. **모델 업데이트**: LangServe는 LLM을 쉽게 업데이트할 수 있는 기능을 제공합니다. 개발자는 새로운 모델 버전으로 업데이트하거나, 다른 모델로 교체할 수 있습니다.\n",
      "\n",
      "4. **접근 제어 및 인증**: LangServe는 접근 제어 및 인증 기능을 제공하여 모델에 대한 접근을 제한하고, 인증된 사용자만 모델을 사용할 수 있도록 합니다.\n",
      "\n",
      "5. **확장성**: LangServe는 수평 확장성을 지원하여 대규모 트래픽을 처리할 수 있습니다. 이를 통해 개발자는 대규모 사용자를 지원하면서도 모델의 성능을 유지할 수 있습니다.\n",
      "\n",
      "6. **다중 모델 지원**: LangServe는 다중 모델을 지원하여 여러 모델을 동시에 배포하고 관리할 수 있습니다. 이를 통해 개발자는 다양한 모델을 테스트하고 비교할 수 있습니다.\n",
      "\n",
      "LangServe를 사용하면 개발자는 LLM을 더 쉽게 배포하고 관리할 수 있으며, 모델의 성능을 모니터링하고 개선할 수 있습니다. LangServe는 오픈 소스 라이브러리이므로 무료로 사용할 수 있으며, 개발자는 LangServe를 자신의 프로젝트에 쉽게 통합할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ccfdb",
   "metadata": {},
   "source": [
    "### LCEL \n",
    "* Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef4d1863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 설명해주세요.\\n    ')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 설명해주세요.\n",
    "    \"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dc4b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL) prompt + llm 연결\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650685f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 방식과 유사합니다. 컴퓨터가 데이터를 통해 배우고, 패턴을 인식하며, 미래의 새로운 데이터에 대해 예측하거나 결정을 내릴 수 있도록 하는 것입니다.\n",
      "\n",
      "쉽게 설명하면, 인공지능 모델의 학습 과정은 다음과 같습니다:\n",
      "\n",
      "1. **데이터 수집**: 인공지능 모델을 학습시키기 위해 필요한 데이터를 수집합니다. 이 데이터는 과거의 경험이나 실제 세계의 관찰로부터 얻어집니다.\n",
      "\n",
      "2. **데이터 전처리**: 수집된 데이터를 모델이 이해하고 학습할 수 있도록 가공합니다. 여기에는 데이터 정리, 변환, 정규화 등의 과정이 포함됩니다.\n",
      "\n",
      "3. **모델 설정**: 학습을 위해 적절한 인공지능 모델을 선택하거나 설계합니다. 이 모델은 신경망, 결정 트리, 서포트 벡터 머신 등 다양한 유형이 있을 수 있습니다.\n",
      "\n",
      "4. **학습**: 모델에 데이터를 입력하여 모델이 데이터의 패턴을 학습하도록 합니다. 이 과정은 보통 '훈련' 또는 '피팅'이라고도 합니다. 모델은 데이터를 통해 각 변수들 사이의 관계를 파악하고, 이 관계를 바탕으로 미래의 새로운 데이터에 대해 예측할 수 있는 능력을 키웁니다.\n",
      "\n",
      "5. **평가**: 학습이 완료된 모델의 성능을 평가합니다. 이는 모델이 새로운, 보지 못한 데이터에 대해 얼마나 잘 예측하는지 테스트하는 과정입니다. 평가를 통해 모델의 성능을 측정하고, 필요하다면 모델을 수정하거나 학습 데이터를 조정할 수 있습니다.\n",
      "\n",
      "6. **튜닝**: 모델의 성능을 최적화하기 위해 모델의 파라미터를 조정하거나 학습 알고리즘을 수정하는 과정입니다.\n",
      "\n",
      "예를 들어, 자율 주행 자동차의 경우, 수많은 도로 상황, 교통 표지판, 보행자의 움직임을 학습하여 실제 도로에서 안전하게 운전할 수 있습니다. 처음에는 운전자가 차를 운전하면서 카메라와 센서를 통해 도로 상황과 운전하는 법을 데이터로 수집합니다. 이 데이터를 통해 인공지능 모델은 자동차가 스스로 운전할 수 있도록 학습하게 됩니다.\n",
      "\n",
      "이처럼 인공지능 모델은 주어진 데이터를 통해 학습하고, 그 학습을 바탕으로 새로운 상황에 대처하는 능력을 발전시킵니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69030b74",
   "metadata": {},
   "source": [
    "### LCEL \n",
    "* Prompt + LLM + OutputParser 을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3ea17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# chain 연결 (LCEL) prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92af5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChain은 다양한 제품과 도구를 제공하여 개발자와 기업이 인공지능(AI) 기술을 쉽게 통합하고 활용할 수 있도록 지원합니다. LangChain의 주요 제품에는 LangSmith, LangServe, LangChain CLI 등이 있습니다.\n",
      "\n",
      "1. **LangSmith**: \n",
      "   - LangSmith는 LangChain에서 제공하는 통합 개발 환경(IDE)입니다. \n",
      "   - 개발자가 랭체인 기반의 애플리케이션을 쉽게 개발, 테스트, 배포할 수 있도록 도와줍니다.\n",
      "   - 주요 기능에는 워크플로우 관리, 디버깅 툴, 에이전트 관리 등이 포함되어 있습니다.\n",
      "\n",
      "2. **LangServe**:\n",
      "   - LangServe는 LangChain에서 제공하는 API 서버입니다. \n",
      "   - 랭체인을 기반으로 하는 언어 모델을 쉽게 배포하고 관리할 수 있도록 지원합니다.\n",
      "   - LangServe를 사용하면 개발자가 모델을 신속하게 프로덕션 환경에 배포하고, 확장성과 성능을 효율적으로 관리할 수 있습니다.\n",
      "\n",
      "3. **LangChain CLI**:\n",
      "   - LangChain CLI는 명령줄 인터페이스를 통해 랭체인의 기능을 활용할 수 있는 도구입니다.\n",
      "   - 사용자들이 랭체인 기반의 애플리케이션을 쉽고 빠르게 구축하고 테스트할 수 있도록 도와줍니다.\n",
      "\n",
      "이러한 제품들은 LangChain이 제공하는 다양한 도구들 중 일부입니다. LangChain은 계속해서 새로운 도구와 기능을 개발하고 있으며, 이를 통해 개발자와 기업이 AI 기술을 보다 쉽게 활용할 수 있도록 지원하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# chain2 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangSmith, LangServe 같은 Product가 있어.\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad7d44",
   "metadata": {},
   "source": [
    "### Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71b2a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리를 쉽게 설명해 드리겠습니다.\n",
      "\n",
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 방식과 유사합니다. 예를 들어, 어린 아이가 고양이를 처음 봤을 때 \"고양이\"라는 단어를 듣고 고양이의 모습을 기억합니다. 그 후, 고양이를 여러 번 더 봤을 때, 아이는 고양이가 무엇인지 더 잘 이해하게 됩니다.\n",
      "\n",
      "인공지능 모델도 이와 유사합니다. 모델이 학습하는 과정을 단계별로 설명해 드리겠습니다.\n",
      "\n",
      "1. **데이터 수집**: 인공지능 모델을 학습시키기 위해서는 많은 데이터가 필요합니다. 예를 들어, 고양이와 아닌 고양이의 사진들을 수집합니다.\n",
      "\n",
      "2. **데이터 전처리**: 수집한 데이터를 모델이 이해할 수 있도록 가공합니다. 예를 들어, 이미지 데이터를 픽셀 단위로 나누어 숫자로 표현합니다.\n",
      "\n",
      "3. **모델 초기화**: 인공지능 모델을 초기화합니다. 모델은 신경망이라는 구조로 되어 있으며, 이 신경망의 가중치와 편향은 랜덤으로 초기화됩니다.\n",
      "\n",
      "4. **학습**: 모델에 데이터를 입력하고, 모델이 예측한 결과와 실제 결과를 비교합니다. 이 과정에서 모델은 오류를 계산하고, 이 오류를 줄이기 위해 가중치와 편향을 조정합니다. 이 과정을 여러 번 반복하면서 모델은 데이터를 학습합니다.\n",
      "\n",
      "5. **최적화**: 모델은 오류를 줄이기 위해 최적화 알고리즘을 사용합니다. 이 알고리즘은 모델의 가중치와 편향을 조정하여 오류를 최소화합니다.\n",
      "\n",
      "6. **평가**: 모델의 성능을 평가합니다. 평가 데이터에 모델을 적용하여 성능을 측정하고, 필요에 따라 모델을 수정합니다.\n",
      "\n",
      "예를 들어, 고양이 분류 모델을 학습시킨다고 가정해 보겠습니다.\n",
      "\n",
      "*   모델에 고양이 사진과 아닌 고양이 사진을 입력합니다.\n",
      "*   모델은 사진을 분석하여 고양이인지 아닌지 예측합니다.\n",
      "*   예측 결과와 실제 결과를 비교하여 오류를 계산합니다.\n",
      "*   모델은 오류를 줄이기 위해 가중치와 편향을 조정합니다.\n",
      "*   이 과정을 여러 번 반복하면서 모델은 고양이를 분류하는 법을 학습합니다.\n",
      "\n",
      "결론적으로, 인공지능 모델의 학습 원리는 데이터 수집, 전처리, 모델 초기화, 학습, 최적화, 평가의 과정으로 이루어져 있습니다. 모델은 데이터를 학습하면서 오류를 줄이고, 성능을 향상합니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    \n",
    "    # 스트리밍 출력\n",
    "    #print(answer)\n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42f1da",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "* 두개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eea9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 한국 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 10문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fd3ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**영화 제목:** 올드보이\n",
      "\n",
      "**영화 줄거리 요약:**\n",
      "\n",
      "1.  주인공 오대수는 15년간 감금된 채 지내게 됩니다.\n",
      "2.  그는 자신이 왜 감금되었고, 누가 자신을 감금했는지 알지 못합니다.\n",
      "3.  어느 날, 갑자기 감금에서 풀려난 오대수는 자신의 과거와 감금의 이유를 찾게 됩니다.\n",
      "4.  오대수는 자신을 감금한 사람에게 복수를 결심합니다.\n",
      "5.  그는 자신의 감금에 대한 단서를 찾기 시작합니다.\n",
      "6.  오대수는 자신의 과거와 관련된 인물들을 만나면서 단서를 추적합니다.\n",
      "7.  그는 자신의 감금에 대한 진실을 조금씩 밝혀냅니다.\n",
      "8.  오대수는 자신을 감금한 사람과 대결하게 됩니다.\n",
      "9.  영화는 긴장감 넘치는 액션과 심리적인 긴장감으로 전개됩니다.\n",
      "10. 결국 오대수는 진실을 알게 되고, 복수를 완성합니다.\n"
     ]
    }
   ],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 입력 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"액션\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f53166",
   "metadata": {},
   "source": [
    "### PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94bbf3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT 모델의 학습 원리는 다음과 같습니다.\n",
      "\n",
      "ChatGPT는 대규모 언어 데이터셋을 기반으로 학습된 인공지능 모델입니다. 이 모델은 주어진 문맥에 따라 다음에 올 가능성이 높은 단어를 예측하도록 학습되며, 이를 통해 자연스러운 대화가 가능해집니다. ChatGPT는 강화 학습을 통해 사용자 피드백을 학습에 활용하여 성능을 지속적으로 개선합니다.\n",
      "\n",
      "ChatGPT 모델의 장점은 다음과 같습니다.\n",
      "\n",
      "* 자연스러운 대화가 가능하여 사용자와의 상호작용이 용이합니다.\n",
      "* 다양한 주제와 분야에 대한 지식을 습득하고 있어 정보 제공에 도움이 됩니다.\n",
      "* 사용자의 질문이나 요청에 빠르게 응답할 수 있습니다.\n",
      "\n",
      "ChatGPT 모델과 비슷한 AI 모델은 다음과 있습니다.\n",
      "\n",
      "* LLaMA\n",
      "* PaLM\n",
      "* BERT\n",
      "* RoBERTa\n",
      "\n",
      "이러한 모델들은 모두 자연어 처리 분야에서 우수한 성능을 발휘하며, ChatGPT와 유사한 방식으로 학습되고 활용됩니다.\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c5bbf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 요약해서 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 요약해서 한국어로 답변해 주세요.', 'llama-4 모델의 학습 원리를 4 문장으로 요약해서 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e64ee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 모델은 대규모 데이터셋을 기반으로 한 머신러닝 알고리즘을 사용하여 학습되며, 자연어 처리 작업에 특화된 트랜스포머 아키텍처를 활용합니다. 이 모델은 주어진 문맥을 이해하고 적절한 응답을 생성하도록 학습되며, 인간과의 대화와 같은 다양한 언어 작업을 수행할 수 있습니다.\n",
      "Gemma 모델은 주어진 문맥을 기반으로 하여 다음에 나타날 토큰을 예측하도록 학습합니다. 이를 위해 대규모의 텍스트 데이터를 학습하고, 각 토큰이 다른 토큰과 얼마나 자주 함께 나타나는지 또는 얼마나 자주 나타나는지를 분석합니다. 이러한 학습을 통해 Gemma는 자연어의 패턴과 구조를 이해하고, 다양한 자연어 처리 작업에 활용할 수 있는 언어 모델을 구축할 수 있습니다.\n",
      "llama-4 모델은 메타에서 개발한 대규모 언어 모델입니다. 이 모델은 방대한 텍스트 데이터를 기반으로 학습되며, 이를 통해 자연어 처리 능력을 습득합니다. 학습 과정에서 모델은 입력된 텍스트의 패턴과 구조를 분석하고, 이를 바탕으로 다음에 나타날 단어 또는 문장을 예측합니다. 이러한 예측과정을 반복하면서 모델은 언어에 대한 이해능력을 향상시키고, 다양한 자연어 처리 작업을 수행할 수 있게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) #AIMessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71d3b8",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "* SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe4d482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "딥러닝은 인공신경망을 사용하여 데이터를 학습하고, 패턴을 발견하며, 의사 결정을 내리는 머신러닝의 한 분야입니다. 딥러닝은 데이터의 복잡한 패턴을 학습하기 위해 다층 신경망을 사용합니다. 이러한 신경망은 여러 개의 레이어로 구성되어 있으며, 각 레이어는 입력 데이터를 처리하고 다음 레이어로 전달합니다.\n",
      "\n",
      "딥러닝의 핵심 아이디어는 데이터의 추상화를 통해 학습하는 것입니다. 즉, 딥러닝 모델은 데이터의 낮은 수준의 특징에서 높은 수준의 특징까지 학습합니다. 예를 들어, 이미지 인식에서 딥러닝 모델은 이미지의 픽셀 데이터를 학습하여 객체의 윤곽선, 모양, 색상 등과 같은 특징을 추출하고, 이를 통해 객체를 인식합니다.\n",
      "\n",
      "딥러닝은 다양한 분야에서 활용되고 있습니다. 예를 들어, 이미지 인식, 음성 인식, 자연어 처리, 추천 시스템 등에 사용됩니다. 딥러닝의 장점으로는 높은 성능, 데이터의 복잡한 패턴 학습 능력, 그리고 대량의 데이터 처리 능력이 있습니다.\n",
      "\n",
      "하지만 딥러닝에는 몇 가지 한계점도 있습니다. 예를 들어, 대량의 데이터가 필요하며, 학습 시간이 오래 걸릴 수 있습니다. 또한, 딥러닝 모델은 해석이 어려울 수 있으며, 과적합 문제가 발생할 수 있습니다.\n",
      "\n",
      "최근에는 딥러닝의 발전으로 인해 다양한 응용 분야에서의 성능이 크게 향상되었습니다. 예를 들어, 자율 주행 자동차, 의료 진단, 금융 거래 등에 사용됩니다. 향후에는 딥러닝의 발전으로 인해 더욱 많은 분야에서 활용될 것으로 예상됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"당신은 {topic} 전문가입니다. 명확하고 자세하게 한국어로 설명해 주세요.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"딥러닝은 무엇인가요?\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c18e37",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "* 예시를 제공 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fa70a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 태양계의 행성\n",
      "1. **수성**: 가장 작은 행성, 태양과 가장 가깝습니다.\n",
      "2. **금성**: 매우 뜨겁고 밝은 행성입니다.\n",
      "3. **지구**: 생명체가 사는 유일한 행성입니다.\n",
      "4. **화성**: 붉은 행성으로, 로봇 탐사가 이루어지고 있습니다.\n",
      "5. **목성**: 태양계에서 가장 큰 행성입니다.\n",
      "6. **토성**: 아름다운 고리를 가진 행성입니다.\n",
      "7. **천왕성**: 자전축이 기울어져 있는 행성입니다.\n",
      "8. **해왕성**: 태양계에서 가장 먼 행성입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "#result = chain.invoke({\"input\": \"양자 얽힘이 무엇인가요?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd92f6e",
   "metadata": {},
   "source": [
    "\n",
    "##### PartialPromptTemplate\n",
    "* 프롬프트의 입력 값에 함수 호출 이나 외부 API를 호출한 동적인 값을 대입할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bae119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 프롬프트: 겨울에 일어나는 대표적인 지구과학 현상은 태풍 발생입니다.\n",
      "🔹 모델 응답: 겨울에 태풍이 발생하는 것은 드문 일입니다. 태풍은 일반적으로 여름과 초가을에 발생합니다. 겨울에는 태풍이 발생하지 않는 것은 아니지만, 극히 드문 일입니다. 겨울에 태풍이 발생하는 경우는 주로 겨울철에 발생하는 저기압이 태풍으로 발달하는 경우입니다. 이러한 경우, 태풍이 발생하는 지역은 주로 열대 지역이 아닌 중위도 지역입니다.\n",
      "\n",
      "겨울에 태풍이 발생하는 대표적인 사례로는 2005년 12월에 발생한 태풍 '카이타크'가 있습니다. 이 태풍은 필리핀 동쪽 해상에서 발생하여 대만과 중국을 강타했습니다.\n",
      "\n",
      "하지만 일반적으로 겨울에는 태풍이 발생하지 않기 때문에, 겨울에 일어나는 대표적인 지구과학 현상으로 태풍 발생을 꼽기는 어렵습니다. 겨울에 일어나는 대표적인 지구과학 현상으로는 북극 소용돌이, 한파, 눈보라 등이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 계절을 결정하는 함수 (남반구/북반구 고려)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "    \n",
    "    if hemisphere == \"north\":  # 북반구 (기본값)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"봄\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"여름\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"가을\"\n",
    "        else:\n",
    "            return \"겨울\"\n",
    "    else:  # 남반구 (계절 반대)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"가을\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"겨울\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"봄\"\n",
    "        else:\n",
    "            return \"여름\"\n",
    "\n",
    "# 프롬프트 템플릿 정의 (부분 변수 적용)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}에 일어나는 대표적인 지구과학 현상은 {phenomenon}입니다.\",\n",
    "    input_variables=[\"phenomenon\"],  # 사용자 입력 필요\n",
    "    partial_variables={\"season\": get_current_season(\"south\")}  # 동적으로 계절 값 할당\n",
    ")\n",
    "\n",
    "# OpenAI 모델 초기화\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# 특정 계절의 현상 질의\n",
    "query = prompt.format(phenomenon=\"태풍 발생\")  # '태풍 발생'은 여름과 관련됨\n",
    "result = model.invoke(query)\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"🔹 프롬프트: {query}\")\n",
    "print(f\"🔹 모델 응답: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a82b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={'info': '1달러 = 1365.14원'} template='현재 {info} 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제 미치는 영향 및 향후에 환율의 예상값에 대한 분석을 제공해 주세요.'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 실시간 환율을 가져오는 함수\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1달러 = {data['rates']['KRW']}원\"\n",
    "\n",
    "# {info} 변수에 API에서 받은 환율 정보를 동적으로 반영\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"현재 {info} 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제 미치는 영향 및 향후에 환율의 예상값에 대한 분석을 제공해 주세요.\",\n",
    "    input_variables=[],  # 사용자 입력 없음\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # API에서 가져온 데이터 자동 반영\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7d8f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 프롬프트: 현재 1달러 = 1365.14원 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제 미치는 영향 및 향후에 환율의 예상값에 대한 분석을 제공해 주세요.\n",
      "🔹 모델 응답: ## 한국 경제에 미치는 영향\n",
      "\n",
      "1.  **수출 증가**: 약한 한국 원화 가치는 한국의 수출을 증가 시킬 수 있습니다. 약한 원화 가치는 한국의 수출 상품을 외국 구매자에게 더 저렴하게 만들어주어 수출량을 증가 시킬 수 있습니다. 이는 한국의 무역 수지 개선에 도움이 될 수 있습니다. 하지만, 수입 물가가 상승하여 인플레이션을 야기 시킬 수 있습니다.\n",
      "2.  **물가 상승**: 원화 약세로 인해 수입 물가가 상승하여 인플레이션을 야기 시킬 수 있습니다. 이는 소비자들의 구매력을 약화시키고, 경제 전반에 걸쳐 물가 상승 압력을 가중 시킬 수 있습니다.\n",
      "3.  **외국인 투자 감소**: 높은 환율은 외국인 투자자들에게 한국 시장에 대한 투자를 매력적으로 만들지 않을 수 있습니다. 이는 외국인 직접 투자 감소로 이어질 수 있으며, 이는 경제 성장에 부정적인 영향을 미칠 수 있습니다.\n",
      "\n",
      "## 향후 환율 예상\n",
      "\n",
      "1.  **글로벌 경제 상황**: 글로벌 경제 상황, 특히 미국의 경제 상황과 통화 정책이 한국 환율에 큰 영향을 미칩니다. 미국 경제가 강세를 보이면, 달러화 강세가 지속될 수 있으며, 이는 원화 약세로 이어질 수 있습니다.\n",
      "2.  **한국은행의 통화 정책**: 한국은행의 통화 정책 결정도 환율에 영향을 미칩니다. 한국은행이 금리 인상에 나설 경우, 원화 강세를 유도할 수 있습니다. 하지만, 금리 동결이나 인하에 나선다면 원화 약세가 지속될 수 있습니다.\n",
      "3.  **대외 변수**: 지정학적 리스크, 국제 유가 변동 등 대외 변수들도 환율에 영향을 미칠 수 있습니다. 예를 들어, 국제 유가가 상승하면, 한국의 수입 물가가 상승하여 원화 약세를 야기할 수 있습니다.\n",
      "\n",
      "환율은 다양한 요인에 의해 변동되며, 예측하기 어려운 변수들이 많습니다. 향후 환율 예상은 이러한 요인들을 고려하여 분석해야 하며, 전문가들의 의견과 경제 지표 등을 참고하여 신중하게 결정해야 합니다.\n"
     ]
    }
   ],
   "source": [
    "# LLM 모델 설정 (GPT-4o-mini 사용)\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# 모델에 프롬프트 전달 및 응답 받기\n",
    "response = model.invoke(prompt.format())\n",
    "\n",
    "# 결과 출력\n",
    "print(\"🔹 프롬프트:\", prompt.format())\n",
    "print(\"🔹 모델 응답:\", response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
